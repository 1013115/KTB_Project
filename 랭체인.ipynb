{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPtf/F4Anjpw2Yz3mED4kE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1013115/KTB_Project/blob/main/%EB%9E%AD%EC%B2%B4%EC%9D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 랭체인\n",
        "- 탄생 배경 : 대규모 언어 모델(LLM)을 활용하여 애플리케이션과 파이프라인을 신속하게 구축할 수 있는 플랫폼의 필요성을 느낌.\n",
        "- 이러한 비전을 가지고 개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 프레임워크 생성\n",
        "\n",
        "### 주요 구성 요소\n",
        "1. 랭체인 라이브러리 : 파이썬과 자바스크립트 라이브러리를 포함하며, 다양한 컴포넌트의 인터페이스와 통합, 이 컴포넌트들을 체인과 에이전트로 결합할 수 잇는 기본 런타임, 체인과 에이전트의 사용 가능한 구현이 가능하다.\n",
        "2. 랭체인 템플릿 : 다양한 작업을 위한 쉽게 배포할 수 있는 참조 아키텍처 모음이다. 이 템플릿은 개발자들이 특정 작업에 맞춰 빠르게 애플리케이션을 구축할 수 있도록한다.\n",
        "3. LangServe : 랭체인 체인을 REST API로 배포할 수 있게 하는 라이브러리입니다. 이를 통해 개발자들은 자신의 애들리케이션을 외부 시스템과 쉽게 통합할 수 있다.\n",
        "4. LangSmith : 개발자 플랫폼으로 LLM 프레임워크에서 구축된 체인을 디버깅, 테스트, 평가, 모니터링할 수 있으며, 랭체인과 원활한 통합을 지원한다.\n",
        "5. LangGraph : LLM을 사용한 상태유지가 가능한 다중 액터 애플리케이션을 구축하기 위한 라이브러리로, LangChain 위에 구축되었으며, LangChain과 함께 사용하도록 설계되었다.\n",
        "\n",
        "### 기본 LLM 체인의 구성 요소\n",
        "\n",
        "1. 프롬프트 : 사용자 또는 시스템에서 제공하는 입력, LLM에게 특정 작업을 수행하도록 요청하는 지시문이다. 프롬프트는 질문, 명령, 문장 시작 부분 등 다양한 형태를 취할 수 있으며, LLM의 응답을 유도하는 데 중요한 역할을 한다.\n",
        "2. **LLM(Large Language Model):** GPT, Gemini 등 대규모 언어 모델로, 대량의 텍스트 데이터에서 학습하여 언어를 이해하고 생성할 수 있는 인공지능 시스템. LLM은 프롬프트를 바탕으로 적절한 응답을 생성하거나, 주어진 작업을 수행하는 데 사용\n",
        "\n",
        "일반적으로 프롬프트 생성 → LLM 처리 → 응답 반환의 순서로 작동한다."
      ],
      "metadata": {
        "id": "v7b2MX6PGA8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip 사용해 설치"
      ],
      "metadata": {
        "id": "iEfqA-iv3U8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxodMogjF2fr",
        "outputId": "c2be4742-ec55-43b3-c661-ee023ca52dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m489.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, requests-toolbelt, jsonpatch, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-core-0.3.10 langchain-text-splitters-0.3.0 langsmith-0.1.134 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "openAI 의존성 패키지 설치\n"
      ],
      "metadata": {
        "id": "m_6bq0fcF_4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pywi5fUSGR5F",
        "outputId": "94fc578c-8aa0-42a3-d4a5-a889e3bb4dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "Successfully installed jiter-0.6.1 openai-1.51.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tiktoken 라이브러리 설치 (openai 모델이 사용하는 토크나이저)\n"
      ],
      "metadata": {
        "id": "CLro9HROGgeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbxp1JroGnPG",
        "outputId": "60e9474d-6d3e-49ff-9857-f6b84f6cce68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "#os.environ['OPENAI_API_KEY'] = 'OPEN_API_KEY'\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "0kqZ2bb5GsDk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "간단한 실습 예제"
      ],
      "metadata": {
        "id": "5fzJgWjIIrVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMtMPwQ3JRZK",
        "outputId": "52b8df85-dc57-45e4-b60d-4214958d1124"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.9 (from langchain-openai)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain-openai)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.9->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-openai\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.10 langchain-openai-0.2.2 langsmith-0.1.134 openai-1.51.2 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o-mini\")\n",
        "llm.invoke('지구의 자전 주기는?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjT6Tn75Irhi",
        "outputId": "5254ca31-c542-4700-b640-f185cf406661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"지구의 자전 주기는 약 24시간, 즉 1일입니다. 정확히 말하면, 지구가 한 바퀴 자전하는 데 걸리는 시간은 약 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 지구의 자전과 태양의 위치 변화를 고려할 때, 우리가 사용하는 태양일은 약 24시간으로 정의됩니다.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 15, 'total_tokens': 105, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-17ded21d-847c-4b9b-b217-5a276b3cdcea-0', usage_metadata={'input_tokens': 15, 'output_tokens': 90, 'total_tokens': 105, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 프롬프트 템플릿 적용\n",
        "- `langchain_core`의 `prompts` 모듈에서 `ChatPromptTemplate` 클래스를 사용하여 질문가로서 질문에 답변하는 형식의 프롬프트 템플릿 생성\n",
        "주어진 질문 `{input}`에 대해 전문가의 관점에서 답변 생성\n",
        "\n",
        "- `ChatPromptTemplate.from_template`메소드는 문자열 형태의 템플릿을 인자로 받아, 해당 형식에 맞는 프롬프트 객체 생성\n",
        "\n",
        "- `<Questioin>: {input}` 부분에서 실제 질문을 받아 답변하도록 요청\n",
        "\n"
      ],
      "metadata": {
        "id": "P4T0FKUBHm3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vSskucSG-j4",
        "outputId": "fed54889-4473-4520-d3b1-142cdab8892d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in astronomy. Answer the question. <Question>: {input}'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt와 LLM 모델을 연결하여 chain 구성\n",
        "- chain을 사용하여 입력된 질문에 대한 답변 생성\n",
        "- 모델의 답변은 `AIMessage` 객체로 제공"
      ],
      "metadata": {
        "id": "b4cejnmOJrWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# chain 연결\n",
        "chain = prompt | llm\n",
        "\n",
        "chain.invoke({\"input\":\"지구의 자전 주기는?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdCKSWKKJqgQ",
        "outputId": "961f77a9-1183-4575-f1a6-38789505ffad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"지구의 자전 주기는 약 24시간입니다. 정확히 말하면, 지구가 한 바퀴 자전하는 데 걸리는 시간은 약 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일반적으로 사용하는 24시간은 태양이 하늘에서 같은 위치에 돌아오는 시간을 기준으로 한 '태양일'입니다. 태양일은 평균적으로 약 24시간으로 정의됩니다.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 29, 'total_tokens': 128, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-5415814b-23d8-4995-b8ea-c866330f7fd5-0', usage_metadata={'input_tokens': 29, 'output_tokens': 99, 'total_tokens': 128, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 최종 문자열 파싱\n",
        "- 프롬프트, LLM, 문자열 출력 파서 (StrOutputParser)를 연결하여 체인 생성\n",
        "- `StrOutputParser`는 모델의 출력을 문자열 형태로 파싱하여 최종 결과 반환"
      ],
      "metadata": {
        "id": "sEXdh4hSKgG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# prompt + model + output parser\n",
        "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy. Answer the question. <Question>: {input}\")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# chain 연결\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "chain.invoke({\"input\":\"지구의 자전 주기는?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "A9hPiduUK4qx",
        "outputId": "a9085546-559e-4101-c3a2-477e64d0e385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'지구의 자전 주기는 약 24시간입니다. 정확히 말하면, 지구가 한 번 자전하는 데 걸리는 시간은 약 23시간 56분 4초로, 이를 \"항성일\"이라고 합니다. 그러나 우리가 일반적으로 사용하는 24시간의 일(日)은 태양일을 기준으로 한 것으로, 이는 지구가 태양을 기준으로 한 위치를 돌아오는 데 필요한 시간입니다. 따라서 지구의 자전 주기는 대략 24시간으로 이해하면 됩니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티체인\n"
      ],
      "metadata": {
        "id": "BkUAOzm3P-bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 순차적인 체인 연결\n",
        "- `ChatPromptTemplate.from_template`를 사용하여 프롬프트를 정의하고, `ChatOpenAI`인스턴스로 GPT 모델을 사용하며, 모델의 출력 파싱\n",
        "- `invoke` 메소드는 입력으로 딕셔너리 객체를 받으며, key는 프롬프트 템플릿에서 정의된 변수명 `({korean_word})`와 일치해야한다.\n",
        "- 아래의 코드에서는 모델이 생성한 번역 결과를 출력함\n"
      ],
      "metadata": {
        "id": "Uu_kUh-9RfxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English.\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\"explain {english_word} using oxford dictionary to me in Korean.\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chain1 = prompt1 | llm | StrOutputParser()\n",
        "\n",
        "chain1.invoke({\"korean_word\":\"미래\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0nri8b69QAaU",
        "outputId": "a194b5cf-a074-48b9-eee9-d7b15ba1e049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Korean word \"미래\" translates to \"future\" in English.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain2 = (\n",
        "    {\"english_word\":chain1} | prompt2 | llm | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"korean_word\":\"미래\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "kSyXGG3eSzXG",
        "outputId": "329d3b7e-373c-4877-a20e-c0a425527786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"미래\"는 영어로 \"future\"로 번역됩니다. 옥스퍼드 사전에서는 \"미래\"를 다음과 같이 설명합니다:\\n\\n미래란, 현재로부터 일정한 시간 이후의 시점을 의미하며, 일어날 가능성이 있는 사건이나 상태를 나타냅니다. 예를 들어, 사람들은 종종 미래에 대한 계획을 세우거나 예측을 합니다. 미래는 불확실성과 가능성으로 가득 차 있으며, 우리의 선택이나 행동에 따라 변화할 수 있습니다. \\n\\n이처럼 \"미래\"는 시간의 흐름 속에서 중요한 개념으로, 우리가 살아가는 방식에 큰 영향을 미치는 요소입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runnable\n",
        ": 사용자가 사용자 정의 체인을 쉽게 생성하고 관리할 수 있도록 설계된 핵심적인 개념\n",
        "이 프로토콜을 통해, 개발자는 일관도니 인터페이스를 사용하여 다양한 타입의 컴포넌트를 조합하고, 복잡한 데이터 처리 파이프라인을 구성할 수 있음\n",
        "<주요 메소드>\n",
        "- **invoke** : 주어진 입력에 대해 체인을 호출하고 그 결과 반환. 단일 입력에 대해 동기적으로 작동\n",
        "- **batch** : 입력 리스트에 대해 체인을 호출하고, 각 입력에 대한 결과를 리스트로 반환. 여러 입력에 대해 동기적으로 작동하며, 효율적인 배치 처리를 가능하게 함\n",
        "- **stream** : 입력에 대해 체인을 호출하고, 결과의 조각들을 스트리밍함. 대용량 데이터 처리나 실시간 데이터 처리에 유용\n",
        "- **비동기 버전** : `ainvoke`, `abatch`, `astream` 등의 비동기 실행 지원 메소드. 더 높은 처리 성능과 효율 달성할 수 있음\n",
        "\n",
        "각 컴포넌트는 입력 및 출력 유형이 명확하게 정의되어있으며 \"Runnable\" 프로토콜을 구현함으로써, 이러한 컴포넌트들은 입력과 출력 스키마를 검사할 수 있다. 이는 개발자가 타입 안정성을 보장하고, 예상치 못한 오류를 방지할 수 있도록 도와줌.\n",
        "\n",
        "<LangChain을 사용하여 커스텀 체인 생성>\n",
        "1. 필요한 컴포넌트를 정의하고 각각 \"Runnable\" 인터페이스 구현\n",
        "2. 컴포넌트들을 조합하여 사용자 정의 체인 생성\n",
        "3. 생성된 체인을 사용하여 데이터처리 작업 수행. 이때 주요 메소드를 사용하여 원하는 방식으로 데이터 처리"
      ],
      "metadata": {
        "id": "WtM6UoJoTYpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"지구과학에서 {topic}에 대해 간단히 설명해주세요.\")\n",
        "model = ChatOpenAI(model = \"gpt-4o-mini\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# invoke 사용\n",
        "result = chain.invoke({\"topic\":\"행성\"})\n",
        "print(\"invoke 결과 :\",result)\n",
        "\n",
        "# batch 사용\n",
        "topics = [\"지구 공전\", \"화산 활동\", \"대륙 이동\"]\n",
        "results = chain.batch([{\"topic\": t} for t in topics])\n",
        "for topic, result in zip(topics, results):\n",
        "    print(f\"{topic} 설명: {result[:50]}...\")  # 결과의 처음 50자만 출력\n",
        "\n",
        "# stream 메소드 사용\n",
        "stream = chain.stream({\"topic\": \"지진\"})\n",
        "print(\"stream 결과:\")\n",
        "for chunk in stream:\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVxfzFMXbPG1",
        "outputId": "e4e9c9c8-9d03-40f5-bcb2-e9ed28c190c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "invoke 결과 행성은 태양계와 같은 천체 시스템에서 별 주위를 공전하는 큰 천체를 의미합니다. 행성은 일반적으로 다음과 같은 특징을 가지고 있습니다:\n",
            "\n",
            "1. **궤도**: 행성은 별 주위를 일정한 궤도로 돌며, 이 궤도는 타원형입니다.\n",
            "\n",
            "2. **질량**: 행성은 충분한 질량을 가지고 있어 중력을 통해 자신의 형태를 구형으로 유지합니다.\n",
            "\n",
            "3. **인접한 천체와의 구분**: 행성은 자신의 궤도에서 다른 천체, 즉 위성이나 소행성과 같은 물체를 청소할 수 있는 능력을 가지고 있습니다.\n",
            "\n",
            "태양계에는 8개의 주요 행성이 있으며, 이들은 내행성(수성, 금성, 지구, 화성)과 외행성(목성, 토성, 천왕성, 해왕성)으로 나누어집니다. 각 행성은 고유한 특성과 환경을 가지고 있어, 지구와 같은 생명체가 존재할 수 있는 가능성도 탐사되고 있습니다.\n",
            "지구 공전 설명: 지구 공전은 지구가 태양을 중심으로 타원형 궤도를 따라 이동하는 과정을 말합니다. 지구는 ...\n",
            "화산 활동 설명: 화산 활동은 지구 내부의 마그마가 지표로 올라오면서 발생하는 현상입니다. 화산은 마그마가 ...\n",
            "대륙 이동 설명: 대륙 이동 이론은 지구의 대륙들이 시간이 지남에 따라 이동해 왔다는 개념입니다. 이 이론의...\n",
            "stream 결과:\n",
            "지진은 지구 내부에서 발생하는 에너지 방출로 인해 발생하는 자연현상입니다. 주로 지각판의 움직임이나 변형에 의해 발생하며, 이러한 움직임은 단층(지각의 균열)에서의 힘이 축적되고 갑작스럽게 방출될 때 일어납니다. 이때 발생하는 에너지는 지진파라는 형태로 전파되어 땅의 진동을 일으킵니다.\n",
            "\n",
            "지진의 강도는 리히터 규모(Richter scale)나 모멘트 규모(Moment magnitude scale)로 측정되며, 진앙(지표면에서 지진이 발생한 지점)과 진원(지진이 발생한 지하의 지점)이라는 개념이 있습니다. 지진은 건물과 인프라에 큰 피해를 줄 수 있으며, 특히 해저 지진은 쓰나미를 유발할 수 있어 더욱 위험합니다.\n",
            "\n",
            "지진은 자연재해 중 하나로, 여러 지역에서 발생할 수 있으며, 빈도와 강도는 지역의 지질 구조와 지각판의 경계에 따라 다릅니다. 이를 통해 지구 내부의 운동과 지각의 변화를 이해하는 데 중요한 단서가 됩니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 비동기 메소드 사용 (async/await 구문 필요)\n",
        "async def run_async():\n",
        "  result = await chain.ainvoke({\"topic\":\"행성\"})\n",
        "  print(\"ainvoke 결과:\",result[:50])\n",
        "\n",
        "asyncio.run(run_async())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXpxATcXcoFE",
        "outputId": "988c1a5c-fd6a-4b6a-e17c-2232c3f97c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ainvoke 결과: 행성은 태양계나 다른 항성계에서 항성을 중심으로 공전하는 천체를 의미합니다. 행성은 일반적\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프롬프트\n",
        "###<작성원칙>\n",
        "\n",
        "**1. 명확성과 구체성**\n",
        "- 질문은 명확하고 구체적이여야한다.\n",
        "\n",
        "**2. 배경 정보를 포함**\n",
        "- 모델이 문맥을 이해할 수 있도록 필요한 배경 정보를 제공하는 것이 좋다.\n",
        "- 이는 환각 현상 발생 위험을 낮추고, 관련성 높은 응답을 생성하는데 도움을 줌\n",
        "\n",
        "**3. 간결함**\n",
        "- 핵심 정보에 초점을 맞추고, 불필요한 정보 배제\n",
        "- 프롬프트가 길어지면 모델이 덜 중요한 부분에 집중하거나 상당한 영향을 받는 문제 발생할 수 있음\n",
        "\n",
        "**4. 열린질문 사용**\n",
        "- 열린 질문을 통해 모델이 자세하고 풍부한 답변을 제공하도록 유도\n",
        "- 단순히 에/아니오로 답할 수 있는 질문보다 더 많은 정보를 제공하는 질문이 좋다.\n",
        "\n",
        "**5. 명확한 목표 설정**\n",
        "- 얻고자 하는 정보나 결과의 유형을 정확하게 정의\n",
        "\n",
        "**6. 언어와 문제**\n",
        "- 대화의 맥락에 적합한 언어와 문체 사용\n",
        "- 모델이 상황에 맞는 표현을 선택하는데 도움이 됨\n"
      ],
      "metadata": {
        "id": "88BKRDpadN3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 프롬프트 템플릿\n",
        ": 단일 문장 또는 간단한 명령을 입력하여 단일 문장 또는 간단한 응답을 생성하는데 사용되는 프롬프트를 구성할 수 있는 문자열 템플릿\n",
        "python의 문자열 포맷팅을 사용하여 동적으로 특정한 위치에 입력 값을 포함시킬 수 있다.\n",
        "1. 구성요소\n",
        "LLM 모델에 입력할 프롬프트를 구성할 때, '지시','예시', '맥락', '질문'과 같은 다양한 구성요소들을 조합할 수 있다.\n",
        "- 지시 : 언어 모델에게 어떤 작업을 수행하도록 요청하는 구체적인 지시\n",
        "- 예시 : 요청된 작업을 수행하는 방법에 대한 하나 이상의 예시.\n",
        "- 맥락 : 특정 작업을 수행하기 위한 추가적인 맥락\n",
        "- 질문 : 어떤 답변을 요구하는 구체적인 질문\n"
      ],
      "metadata": {
        "id": "SrNLcBXHeb5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 문자열 템플릿\n",
        "- PromptTemplate.from_template 메서드를 사용하여 문자열 템플릿으로부터 PromptTemplate인스턴스 생성. 이때, template_text 변수에 정의된 템플릿 문자열 사용됨\n",
        "- 생성된 PromptTemplate 인스턴스의 format 메서드를 사용하여, 실제 'name'과 'age'값으로 템플릿에 채워서 프롬프트 구성\n",
        "- 결과적으로 filled_prompt 변수에는 완성된 프롬프트 문자열 저장"
      ],
      "metadata": {
        "id": "I72F0pymxH7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template_text = \"안녕하세요, 제 이름은 {name}이고, 나이는 {age}입니다.\"\n",
        "prompt_template = PromptTemplate.from_template(template_text)\n",
        "filled_prompt = prompt_template.format(name=\"홍길동\", age=30)\n",
        "filled_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4i3PbZpieVwP",
        "outputId": "0d91a712-b8e1-4160-883a-ab6d4254c05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요, 제 이름은 홍길동이고, 나이는 30입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 프롬프트 템플릿 간의 결합\n",
        "- promptTemplate 클래스는 문자열을 기반으로 프롬프트 템플릿을 생성하고, + 연산자를 사용하여 직접 결합하는 동작 지원\n",
        "- PromptTemplate 인스턴스 간의 직접적인 결합뿐만아니라, 이들 인스턴스와 문자열로 이루어진 템플릿을 결합하여 새로운 PromptTemplate 인스턴스를 생성하는것도 가능하다.\n"
      ],
      "metadata": {
        "id": "8NJruN6WyPym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_prompt = (\n",
        "    prompt_template\n",
        "    + PromptTemplate.from_template(\"\\n\\n아버지를 아버지라 부를수 없습니다\")\n",
        "    + \"\\n\\n{language}로 번역해주세요\"\n",
        ")\n",
        "combined_prompt.format(name=\"홍길동\", age=30, language=\"영어\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "UXgclEICywV4",
        "outputId": "02a19dad-881f-47f1-de68-0f5269733a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요, 제 이름은 홍길동이고, 나이는 30입니다.\\n\\n아버지를 아버지라 부를수 없습니다\\n\\n영어로 번역해주세요'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = combined_prompt | llm | output_parser\n",
        "chain.invoke({\"name\":\"홍길동\", \"age\":30, \"language\":\"영어\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "wMo1wxFlzb9U",
        "outputId": "a21ee8bd-9f22-4eab-f644-295c579bc206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, my name is Hong Gil-dong, and I am 30 years old.\\n\\nI cannot call my father \"father.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 챗 프롬프트 템플릿\n",
        "입력은 여러 메시지를 원소로 갖는 list 로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성된다.\n",
        "1. Message 유형\n",
        "- SystemMessage: 시스템의 기능 설명\n",
        "- HumanMessage: 사용자의 질문을 나타냄\n",
        "- AIMessage: AI 모델의 응답 제공\n",
        "- FunctionMessage: 특정 함수 호출의 결과를 나타냄\n",
        "- ToolMessage: 도구 호출의 결과를 나타냄\n",
        "\n",
        "2. 2-튜플 형태의 메시지 리스트\n",
        "- ChatPromptTemplate.from_messsage 메서드를 사용하여 메시지 리스트로부터 ChatPromptTemplate 인스턴스를 생성하는 방식은 대화형 프롬프트를 생성하는데 유용하다.\n",
        "- 2-튜플 형태의 메시지 리스트를 입력받아, 각 메시지의 역할(type)과 내용(content)을 기반으로 프롬프트 구성"
      ],
      "metadata": {
        "id": "D3vEOy9Cz465"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    (\"user\",\"{user_input}\"),\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가?\")\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsxtd0uQ1dsU",
        "outputId": "72054b26-61c6-463e-fdd4-d1f79186fa6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = chat_prompt |llm|StrOutputParser()\n",
        "\n",
        "#chain.invoke({\"user_input:태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
        "chain.invoke({\"user_input\":\"태양계에서 가장 큰 행성은 무엇인가요?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XEMavNrA2OqF",
        "outputId": "684472bd-dab1-4526-eba9-cd89094e6ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지구의 약 1,300배 정도 되는 부피를 가지고 있으며, 그 크기와 질량 덕분에 태양계 내의 다른 행성들에 비해 압도적인 존재감을 가지고 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MessagePromptTemplate 활용\n"
      ],
      "metadata": {
        "id": "KIxpgj-C2i5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
        "])\n",
        "\n",
        "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가?\")\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK_YZ0k02sbj",
        "outputId": "fd8aa1bd-29e1-454f-af3c-0bc10e330494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='이 시스템은 천문학 질문에 답변할 수 있습니다.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='태양계에서 가장 큰 행성은 무엇인가?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = chat_prompt | llm | StrOutputParser()\n",
        "chain.invoke({\"user_input\":\"태양계에서 가장 큰 행성은 무엇인가요?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "4exUF-Bf3Gw1",
        "outputId": "beace5b1-1790-4612-aa10-4450491c025a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 142,984킬로미터로, 태양계의 다른 모든 행성보다 훨씬 큽니다. 또한 목성은 가스 거성으로, 두꺼운 대기와 강력한 자기장을 가지고 있습니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-Shot Prompt\n",
        "- 언어모델에 몇 가지 예시를 제공하여 특정 작업을 수행하도록 유도하는 기법. 모델의 성능을 크게 향상 시킬 수 있다.\n",
        "- 주어진 예제들을 참고하여 더 정확하고 일관된 응답을 생성할 수 있다. 특히 특정 도메인이나 형식의 질문에 대해 모델의 성능을 향상시키는 데 효과적이다.\n",
        "1. Few-shot 예제 포맷터 생성\n"
      ],
      "metadata": {
        "id": "YVrxG6VR3TMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate\n",
        "\n",
        "example_prompt = PromptTemplate.from_template(\"질문 : {question}\\n{answer}\")"
      ],
      "metadata": {
        "id": "GebrPbNC4nf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 예제 세트 생성"
      ],
      "metadata": {
        "id": "yFgiN-aB4zTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\",\n",
        "        \"answer\": \"지구 대기의 약 78%를 차지하는 질소입니다.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"광합성에 필요한 주요 요소들은 무엇인가요?\",\n",
        "        \"answer\": \"광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"피타고라스 정리를 설명해주세요.\",\n",
        "        \"answer\": \"피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"지구의 자전 주기는 얼마인가요?\",\n",
        "        \"answer\": \"지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"DNA의 기본 구조를 간단히 설명해주세요.\",\n",
        "        \"answer\": \"DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"원주율(π)의 정의는 무엇인가요?\",\n",
        "        \"answer\": \"원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다.\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "2f6uLPfL41jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. FewShotTemplate 생성"
      ],
      "metadata": {
        "id": "yzsq4qjG46Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate\n",
        "\n",
        "# FewShotPromptTemplate 생성\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,  # 예제 포맷팅에 사용할 템플릿\n",
        "    suffix = \"질문 : {input}\",         # 예제 뒤에 추가될 접미사\n",
        "    input_variables = [\"input\"],      # 입력 변수 저장\n",
        ")\n",
        "\n",
        "# 새로운 질문에 대한 프롬프트 생성하고 출력\n",
        "print(prompt.invoke({\"input\":\"화성의 표면이 붉은 이유는 무엇인가요?\"}).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyqsQdyC4_AO",
        "outputId": "0b49e268-11e0-4ad8-ecab-ce2cad15c1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문 : 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\n",
            "지구 대기의 약 78%를 차지하는 질소입니다.\n",
            "\n",
            "질문 : 광합성에 필요한 주요 요소들은 무엇인가요?\n",
            "광합성에 필요한 주요 요소는 빛, 이산화탄소, 물입니다.\n",
            "\n",
            "질문 : 피타고라스 정리를 설명해주세요.\n",
            "피타고라스 정리는 직각삼각형에서 빗변의 제곱이 다른 두 변의 제곱의 합과 같다는 것입니다.\n",
            "\n",
            "질문 : 지구의 자전 주기는 얼마인가요?\n",
            "지구의 자전 주기는 약 24시간(정확히는 23시간 56분 4초)입니다.\n",
            "\n",
            "질문 : DNA의 기본 구조를 간단히 설명해주세요.\n",
            "DNA는 두 개의 폴리뉴클레오티드 사슬이 이중 나선 구조를 이루고 있습니다.\n",
            "\n",
            "질문 : 원주율(π)의 정의는 무엇인가요?\n",
            "원주율(π)은 원의 지름에 대한 원의 둘레의 비율입니다.\n",
            "\n",
            "질문 : 화성의 표면이 붉은 이유는 무엇인가요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 예제 선택하기\n",
        "- 의미적 유사성을 기반으로 가장 관련성 높은 예제를 선택\n",
        "- SemanticSimilarityExampleSelector는 입력 질문과 가장 유사한 예제를 선택"
      ],
      "metadata": {
        "id": "qfl0m7rs6d_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdTp8Xgz7pzX",
        "outputId": "5c5a3b48-22be-4ac3-a70b-290bee8cf8ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma)\n",
            "  Downloading chromadb-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain-chroma)\n",
            "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1.40 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (0.3.10)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-chroma) (1.26.4)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvicorn-0.31.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.5)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.7)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.9.2)\n",
            "Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1,>=0.95.2->langchain-chroma)\n",
            "  Downloading starlette-0.39.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (0.1.134)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.1.40->langchain-chroma) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1.40->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.1.40->langchain-chroma) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.3)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
            "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (71.0.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2024.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.3.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
            "Downloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading chromadb-0.5.13-py3-none-any.whl (602 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m603.0/603.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
            "Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.39.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.31.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading uvloop-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=30fcb0663bc918f75a3c8aeef94dd2719d06f632726a514ec8036b626c9c1e6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, python-dotenv, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb, langchain-chroma\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.13 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.2 httptools-0.6.1 humanfriendly-10.0 kubernetes-31.0.0 langchain-chroma-0.1.4 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.19.2 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-util-http-0.48b0 overrides-7.7.0 posthog-3.7.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.39.2 uvicorn-0.31.1 uvloop-0.20.0 watchfiles-0.24.0 websockets-13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    Chroma,   # 벡터저장소\n",
        "    k=1,  # 선택할 예제 수\n",
        ")\n",
        "\n",
        "\n",
        "question = \"화성의 표면이 붉은 이유는 무엇인가요?\"\n",
        "selected_examples = example_selector.select_examples({\"question\": question})\n",
        "print(f\"입력과 가장 유사한 예제 : {question}\")\n",
        "for example in selected_examples:\n",
        "    print(\"\\n\")\n",
        "    for k, v in example.items():\n",
        "        print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZuCzojX6pVe",
        "outputId": "f7acec09-470c-4ed3-a0ab-eaaf545462b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력과 가장 유사한 예제 : 화성의 표면이 붉은 이유는 무엇인가요?\n",
            "\n",
            "\n",
            "answer: 지구 대기의 약 78%를 차지하는 질소입니다.\n",
            "question: 지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 채팅 모델에서 Few-shot 예제 사용하기\n",
        "1. 고정 예제 사용하기\n",
        "모델이 제공된 예제 스타일에 따라 간결하게 답변 (00을 물어보면 일단 ~~~로 답변)"
      ],
      "metadata": {
        "id": "i33T6eLg783H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "# 예제 정의\n",
        "examples = [\n",
        "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
        "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
        "]\n",
        "\n",
        "# 예제 프롬프트 템플릿 정의\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Few-shot 프롬프트 템플릿 생성\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "# 최종 프롬프트 템플릿 생성\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 모델과 체인 생성\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "chain = final_prompt | model\n",
        "\n",
        "# 모델에 질문하기\n",
        "result = chain.invoke({\"input\": \"지구의 자전 주기는 얼마인가요?\"})\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiRFU1iw7oJT",
        "outputId": "9787bb5f-3eaf-4797-c81f-c580ce58eaaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "지구의 자전 주기는 약 24시간, 즉 1일입니다. 정확히는 약 23시간 56분 4초로, 이를 '항성일'이라고 합니다. 그러나 우리가 일반적으로 사용하는 1일은 태양일로, 태양이 같은 위치에 돌아오는 시간을 기준으로 합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 동적 Few-Shot 프롬프팅\n",
        "\n",
        "ExampleSelector를 사용하여 입력에 따라 전체 예제 세트에서 가장 관련성 높은 예제만 선택해서 보여줌"
      ],
      "metadata": {
        "id": "87HJxMgV8iQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# 더 많은 예제 추가\n",
        "examples = [\n",
        "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
        "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
        "    {\"input\": \"피타고라스 정리를 설명해주세요.\", \"output\": \"직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다.\"},\n",
        "    {\"input\": \"DNA의 기본 구조를 간단히 설명해주세요.\", \"output\": \"DNA는 이중 나선 구조를 가진 핵산입니다.\"},\n",
        "    {\"input\": \"원주율(π)의 정의는 무엇인가요?\", \"output\": \"원의 둘레와 지름의 비율입니다.\"},\n",
        "]\n",
        "\n",
        "# 벡터 저장소 생성\n",
        "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
        "\n",
        "# 예제 선택기 생성\n",
        "example_selector = SemanticSimilarityExampleSelector(\n",
        "    vectorstore=vectorstore,\n",
        "    k=2,\n",
        ")\n",
        "\n",
        "# Few-shot 프롬프트 템플릿 생성\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=ChatPromptTemplate.from_messages(\n",
        "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 최종 프롬프트 템플릿 생성\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
        "        few_shot_prompt,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 모델과 체인 생성\n",
        "chain = final_prompt | ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "# 모델에 질문하기\n",
        "result = chain.invoke({\"input\":\"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtoFwARV8lCo",
        "outputId": "21e9a7d7-73fc-496c-8a57-5e4245b0cf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 142,984킬로미터로, 태양계의 다른 행성들보다 훨씬 큽니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partial Prompt\n",
        "프롬프트 템플릿을 부분적으로 포맷팅하는 것은 함수에 일부 인자만 바인딩하는 것과 유사하다. 필요한 값의 일부만 미리 입력하여 새로운 프롬프트 템플릿을 만드는 방법이다.\n",
        "복잡한 프롬프트를 관리하거나, 동적인 값을 효율적으로 처리해하할 때 유용\n",
        "\n",
        "\n",
        "여기에는 2가지 방법이 존재한다,\n",
        "1. 문자열 값을 사용한 부분 포맷팅"
      ],
      "metadata": {
        "id": "oESsh2EN9d3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# 기본 프롬프트 템플릿 정의\n",
        "prompt = PromptTemplate.from_template(\"지구의 {layer}에서 가장 흔한 원소는 {element}입니다.\")\n",
        "\n",
        "# 'layer' 변수에 '지각' 값을 미리 지정하여 부분 포맷팅\n",
        "partial_prompt = prompt.partial(layer=\"지각\")\n",
        "\n",
        "# 나머지 'element' 변수만 입력하여 완전한 문장 생성\n",
        "print(partial_prompt.format(element=\"산소\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS8cGczR-Mnm",
        "outputId": "1ce8d9e0-794b-4ef9-a7a2-6575a5d7e759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "지구의 지각에서 가장 흔한 원소는 산소입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 초기화 시 부분 변수 지정\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"지구의 {layer}에서 가장 흔한 원소는 {element}입니다.\",\n",
        "    input_variables=[\"element\"],  # 사용자 입력이 필요한 변수\n",
        "    partial_variables={\"layer\": \"맨틀\"}  # 미리 지정된 부분 변수\n",
        ")\n",
        "\n",
        "# 남은 'element' 변수만 입력하여 문장 생성\n",
        "print(prompt.format(element=\"규소\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTdhTmy_-SMy",
        "outputId": "d3d6a58e-8eb3-4783-daba-96dd90d4989c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "지구의 맨틀에서 가장 흔한 원소는 규소입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 함수를 사용한 부분 포맷팅\n",
        "- 현재 계절을 동적으로 결정"
      ],
      "metadata": {
        "id": "wbmpUle2-oaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# 현재 계절을 반환하는 함수 정의\n",
        "def get_current_season():\n",
        "    month = datetime.now().month\n",
        "    if 3 <= month <= 5:\n",
        "        return \"봄\"\n",
        "    elif 6 <= month <= 8:\n",
        "        return \"여름\"\n",
        "    elif 9 <= month <= 11:\n",
        "        return \"가을\"\n",
        "    else:\n",
        "        return \"겨울\"\n",
        "\n",
        "# 함수를 사용한 부분 변수가 있는 프롬프트 템플릿 정의\n",
        "prompt = PromptTemplate(\n",
        "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}입니다.\",\n",
        "    input_variables=[\"phenomenon\"],  # 사용자 입력이 필요한 변수\n",
        "    partial_variables={\"season\": get_current_season}  # 함수를 통해 동적으로 값을 생성하는 부분 변수\n",
        ")\n",
        "\n",
        "# 'phenomenon' 변수만 입력하여 현재 계절에 맞는 문장 생성\n",
        "print(prompt.format(phenomenon=\"냑엽\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAnNg_5k-tPm",
        "outputId": "04bf1452-d139-419a-bb3f-ab3d5bc56034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가을에 일어나는 대표적인 지구과학 현상은 냑엽입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM\n",
        "주로 단일 요청에 대한 복잡한 출력을 생성하는데 적합\n",
        "- 기능 : LLM 클래스는 텍스트 문자열을 입력으로 받아 처리한 후, 텍스트 문자열을 반환함. 이 모델은 광범휘한 언어 이해 및 텍스트 생성 작업에 사용됨. 예를 들어, 문서요약, 콘텐츠 생성, 질문에 대한 답변생성 등 복잡한 자연어처리 작업을 수행할 수 있음\n",
        "- 예시 : 사용자가 특정 주제에 대한 설명을 요청할 때, LLM은 주어진 택스트 입력을 바탕으로 상세한 설명을 생성하여 반환할 수 있다."
      ],
      "metadata": {
        "id": "A6N_peKI_kWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 랭체인이 직접 LLM을 제공하는 것이 아니라, OpenAI, Cohere, Hugging Face 등과 같은 다양한 LLM 제공 업체로부터 모델을 사용할 수 있게 하는 플랫폼 역할을 한다\n",
        "\n",
        " <LLM interface 특징>\n",
        " - **표준화된 인터페이스**: 랭체인의 LLM 클래스는 사용자가 문자열을 입력으로 제공하면, 그에 대한 응답으로 문자열을 반환하는 표준화된 방식을 제공. 이는 다양한 LLM 제공 업체 간의 호환성을 보장하며, 사용자는 복잡한 API 변환 작업 없이 여러 LLM을 쉽게 탐색하고 사용할 수 있다.\n",
        "\n",
        "- **다양한 LLM 제공 업체 지원**: 랭체인은 OpenAI의 GPT 시리즈, Cohere의 LLM, Hugging Face의 Transformer 모델 등 다양한 LLM 제공 업체와의 통합을 지원. 이를 통해 사용자는 자신의 요구 사항에 가장 적합한 모델을 선택하여 사용할 수 있다."
      ],
      "metadata": {
        "id": "Av4NYZGgAfLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "llm.invoke(\"한국의 대표적인 관광지 3군데를 추천해주세요.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "EJJoAc6M-tQp",
        "outputId": "0c348574-acaf-4ed3-8702-839043a249a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. 경복궁\\n- 서울에 위치한 대한민국의 궁궐 중 가장 크고 오래된 역사적인 유적지이다.\\n- 조선 왕조의 본궁으로 이용되었으며, 현재는 국립고궁박물관으로 운영되고 있다.\\n- 아름다운 건축물과 정원이 인상적이며, 한국의 전통문화를 체험할 수 있는 다양한 행사가 열린다.\\n\\n2. 제주도\\n- 한국의 가장 남쪽에 위치한 섬으로, 제주특별자치도로 지정되어 있다.\\n- 아름다운 자연경관과 다양한 역사유적, 문화유산이 풍부하다.\\n- 해안선을 따라 우도, 성산일출봉,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatModel\n",
        "사용자와의 상호작용을 통한 연속적인 대화 관리에 더 적합\n",
        "- 기능: Chat Model 클래스는 메시지의 리스트를 입력으로 받고, 하나의 메시지를 반환합니다. 이 모델은 대화형 상황에 최적화되어 있으며, 사용자와의 연속적인 대화를 처리하는 데 사용됩니다. Chat Model은 대화의 맥락을 유지하면서 적절한 응답을 생성하는 데 중점을 둡니다."
      ],
      "metadata": {
        "id": "6p2N3Uh_AKp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Model 클래스는 랭체인에서 중요한 구성 요소로, 대화형 메시지를 입력으로 사용하고 대화형 메시지를 출력으로 반환하는 특수화된 LLM 모델. 일반 텍스트를 사용하는 대신 대화의 맥락을 포함한 메시지를 처리하며, 이를 통해 보다 더 자연스러운 대화를 가능하게 함.\n",
        "\n",
        "<chat model interface 특징>\n",
        "- **대화형 입력과 출력**: Chat Model은 대화의 연속성을 고려하여 입력된 메시지 리스트를 기반으로 적절한 응답 메시지 생성. (챗봇, 가상 비서, 고객 지원 시스템 등 대화 기반 서비스)\n",
        "\n",
        "- **다양한 모델 제공 업체와의 통합**: 랭체인은 OpenAI, Cohere, Hugging Face 등 다양한 모델 제공 업체와의 통합을 지원. 이를 통해 개발자는 여러 소스의 Chat Models를 조합하여 활용할 수 있다.\n",
        "\n",
        "- **다양한 작동 모드 지원**: 동기(sync), 비동기(async), 배치(batching), 스트리밍(streaming) 모드에서 모델을 사용할 수 있는 기능 제공. 다양한 애플리케이션 요구사항과 트래픽 패턴에 따라 유연한 대응 가능."
      ],
      "metadata": {
        "id": "K5jEKbJTBCyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI()\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 여행 전문가입니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "chain = chat_prompt | chat | StrOutputParser()\n",
        "chain.invoke({\"user_input\": \"안녕하세요? 한국의 대표적인 관광지 3군데를 추천해주세요.\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "56U9ZnepBqmh",
        "outputId": "6c2229fa-8540-40f2-d77c-9693609754c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요! 한국의 대표적인 관광지로는 다음 세 곳을 추천해 드립니다:\\n\\n1. 경복궁 (Gyeongbokgung): 서울에 위치한 경복궁은 조선 시대의 궁궐로서 한국의 역사와 전통을 경험할 수 있는 곳입니다. 아름다운 건물과 정원, 전통 복식을 입은 경비병들이 볼만한 관광지입니다.\\n\\n2. 부산 해운대해수욕장 (Haeundae Beach): 부산에 위치한 해운대해수욕장은 한국에서 가장 유명한 해변 중 하나로서 탁 트인 바다와 깨끗한 모래사장을 즐길 수 있습니다. 여름에는 수영을 즐기고, 가을에는 해변 산책을 즐길 수 있는 곳입니다.\\n\\n3. 경주 (Gyeongju): 경주는 한국의 역사적인 도시로서 수많은 유적지와 문화유산이 있는 곳입니다. 석굴암, 불국사, 첨성대 등의 유명한 관광지뿐만 아니라 아름다운 자연 풍경도 즐길 수 있는 곳입니다.\\n\\n이 세 곳은 한국을 대표하는 관광지이며, 각자의 매력을 가지고 있습니다. 즐겁고 풍부한 여행되시길 바랍니다!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain의 LLM 모델 파라미터 설정\n",
        "\n",
        "- Temperature: 생성된 텍스트의 다양성 조정. 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력 생성.\n",
        "- Max Tokens (최대 토큰 수): 생성할 최대 토큰 수 지정. 생성할 텍스트의 길이 제한.\n",
        "- Top P (Top Probability): 생성 과정에서 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식. 출력의 다양성을 조정하는 데 도움.\n",
        "- Frequency Penalty (빈도 패널티): 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률 감소시킴. 반복을 줄이고 텍스트의 다양성을 증가시킬 수 있다. (0~1)\n",
        "- Presence Penalty (존재 패널티): 텍스트 내에서 단어의 존재 유무에 따라 그 단어의 선택 확률 조정. 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됨. (0~1)\n",
        "- Stop Sequences (정지 시퀀스): 특정 단어나 구절이 등장할 경우 생성을 멈추도록 설정. 이는 출력을 특정 포인트에서 종료하고자 할 때 사용됨.\n"
      ],
      "metadata": {
        "id": "1_lOWe2fB-zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성단계에서 설정\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "params = {\n",
        "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
        "    \"max_tokens\": 100,          # 생성할 최대 토큰 수\n",
        "}\n",
        "\n",
        "kwargs = {\n",
        "    \"frequency_penalty\": 0.5,   # 이미 등장한 단어의 재등장 확률\n",
        "    \"presence_penalty\": 0.5,    # 새로운 단어의 도입을 장려\n",
        "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
        "\n",
        "}\n",
        "\n",
        "# 모델 인스턴스를 생성할 때 설정\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", **params, model_kwargs = kwargs)\n",
        "\n",
        "\n",
        "# 모델 호출\n",
        "question = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
        "response = model.invoke(input=question)\n",
        "\n",
        "# 전체 응답 출력\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwjTcYQPB99-",
        "outputId": "7ba7e287-22e7-416d-9a4d-3ea03523b3e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: Parameters {'stop', 'frequency_penalty', 'presence_penalty'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 139,822킬로미터로, 태양계의 다른 모든 행성을 합친 것보다도 큽니다. 또한 목성은 가스 거인으로 분류되며, 두꺼운 대기와 많은 위성을 가지고 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 20, 'total_tokens': 96, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None} id='run-f1b48f7e-4a9e-438c-a8e6-4fa880b3f3c4-0' usage_metadata={'input_tokens': 20, 'output_tokens': 76, 'total_tokens': 96, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 호출 단계에서 invoke를 사용하여 설정\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "params = {\n",
        "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
        "    \"max_tokens\": 10,          # 생성할 최대 토큰 수\n",
        "}\n",
        "\n",
        "# 모델 인스턴스를 호출할 때 전달\n",
        "response = model.invoke(input=question, **params)\n",
        "\n",
        "# 문자열 출력\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ryGb_ZCxDv",
        "outputId": "52741432-e94e-4d1f-ca6d-f9dd4f88c0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "태양계에서 가장 큰 행성은 목\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 모델 파라미터를 추가로 바인딩 (bind 메소드)\n",
        "\n",
        "bind 메소드를 사용하여 모델 인스턴스에 파라미터를 추가로 제공\n",
        "- 장점 : 특정 모델 설정을 기본값으로 사용하고자 할 때 유용하며, 특수한 상황에서 일부 파라미터를 다르게 적용하고 싶을 때 사용\n",
        "- 일관된 파라미터 설정을 유지하면서 상황에 맞는 유연한 대응 가능\n"
      ],
      "metadata": {
        "id": "wRk_7A21DGsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
        "    (\"user\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=100)\n",
        "\n",
        "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇인가요?\")\n",
        "\n",
        "before_answer = model.invoke(messages)\n",
        "\n",
        "# # binding 이전 출력\n",
        "print(before_answer)\n",
        "\n",
        "# 모델 호출 시 추가적인 인수를 전달하기 위해 bind 메서드 사용 (응답의 최대 길이를 10 토큰으로 제한)\n",
        "chain = prompt | model.bind(max_tokens=10)\n",
        "\n",
        "after_answer = chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇인가요?\"})\n",
        "\n",
        "# binding 이후 출력\n",
        "print(after_answer)\n"
      ],
      "metadata": {
        "id": "AEg4-wcUDkqk",
        "outputId": "36b0debd-ab39-48ec-f721-d6ff63a4f271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='태양계에서 가장 큰 행성은 목성(Jupiter)입니다. 목성은 지름이 약 139,820킬로미터로, 태양계의 다른 모든 행성보다 훨씬 큽니다. 또한, 목성은 가스 거인으로 분류되며, 두꺼운 대기와 강력한 자기장을 가지고 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 38, 'total_tokens': 116, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None} id='run-323039df-6631-4f1b-b637-40a5fd0acbc1-0' usage_metadata={'input_tokens': 38, 'output_tokens': 78, 'total_tokens': 116, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
            "content='태양계에서 가장 큰 행성은 목' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 38, 'total_tokens': 48, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'length', 'logprobs': None} id='run-04af58d8-c8b9-44d2-a939-3ed5b04026e5-0' usage_metadata={'input_tokens': 38, 'output_tokens': 10, 'total_tokens': 48, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 출력 파서\n",
        "\n",
        "모델의 출력을 처리하고, 그 결과를 원하는 형식으로 변환하는 역할을 한다. 출력 파서는 모델에서 반환된 원시 텍스트를 분석하고, 특정 정보를 추출하거나, 출력을 특정형식으로 재구성하는데 사용.\n",
        "\n",
        "<주요기능>\n",
        "- 출력 포맷 변경 : 모델의 출력을 사용자가 원하는 형식으로 변환\n",
        "- 정보 추출 : 원시 텍스트 출력에서 필요한 정보를 추출\n",
        "- 결과 정제 : 모델 출력에서 불필요한 정보를 제거하거나, 응답을 더 명확하게 만드는 등의 후처리 작업 수행\n",
        "- 조건부 로직 적용 : 출력 데이터를 기반으로 특정 조건에 따라 다른 처리를 수행함. 예를 들어, 모델의 응답에 따라 사용자에게 추가 질문을 하거나, 다른 모델을 호출할 수 있음\n"
      ],
      "metadata": {
        "id": "r2VwnMK1n7yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV Parser\n",
        "\n",
        "- `CommaSeparatedListOutputParser`를 사용하여, 모델이 생성한 텍스트에서 쉼표로 구분된 항목을 추출하여 리스트 형태로 정리하여 파싱\n",
        "\n",
        "- `get_format_instructions` 메소드를 호출하여 모델에 전달할 포맷 지시사항을 얻는다"
      ],
      "metadata": {
        "id": "etv4swqTorHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77Cf-keQpSQC",
        "outputId": "15c98186-4f64-4f9b-f8fe-e95e94010b07"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `PromptTemplate`를 사요하여 사용자 입력에 기반한 프롬프트 생성\n",
        "- `prompt`, `model`, `output_parser` 를 파이프 연산자를 사요하여 연결하여 체인 구성\n",
        "  \n",
        "  이 체인은 사용자의 입력을 받아 프롬프트를 생성하고, 생성된 프롬프트를 모델에 전달한 후, 모델의 출력을 파싱하는 과정을 순차적으로 수행\n",
        "\n",
        "- `temperature` 설정은 모델이 보다 더 일관된 출력을 생성하도록 한다."
      ],
      "metadata": {
        "id": "H3Z8K5MHpee5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"List five {subject}.\\n{format_instructions}\",\n",
        "    input_variables = [\"subject\"],\n",
        "    partial_variables = {\"format_instructions\": format_instructions},\n",
        "\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "chain.invoke({\"subject\": \"ice cream flavors\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJpnqXukqBKG",
        "outputId": "665a721c-3deb-408a-9b32-d3b9e0a53a24"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vanilla',\n",
              " 'chocolate',\n",
              " 'strawberry',\n",
              " 'mint chocolate chip',\n",
              " 'cookies and cream']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON Parser\n",
        "\n",
        "랭체인의 `JSONOutputParser`와 Pydantic을 사용하여, 모델 출력을 JSON 형식으로 파싱하고 Pydantic 모델로 구조화하는 과정\n",
        "\n",
        "- `JsonOutputParser`는 모델의 출력을 JSON으로 해석하고, 지정된 Pydantic 모델(`CusineRecipe` 클래스)에 맞게 데이터를 구조화하여 제공\n"
      ],
      "metadata": {
        "id": "VVnRhGb_qA8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i35Fgve3t1b3",
        "outputId": "ca77ff26-16cd-4d1c-fa1b-980dc36da6b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import pydantic\n",
        "\n",
        "# 자료구조 정의 (pydantic)\n",
        "class CusineRecipe(BaseModel):\n",
        "  name: str = Field(description=\"name of a cusine\")\n",
        "  recipe: str = Field(description=\"recipe to cook the cusine\")\n",
        "\n",
        "output_parser = JsonOutputParser(pydantic_object=CusineRecipe)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gArUR7HErMod",
        "outputId": "a5e71c2b-5a24-4116-d847-ce808ea42a27"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"Answer the user query.\\n{format_instructions}\\n{user_input}\\n\",\n",
        "    input_variables = [\"user_input\"],\n",
        "    partial_variables = {\"format_instructions\":format_instructions},\n",
        ")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QfJLZ9YuRMC",
        "outputId": "3fe60d0e-bbb9-45d4-aa06-c56bf33564a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['user_input'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"name of a cusine\", \"type\": \"string\"}, \"recipe\": {\"title\": \"Recipe\", \"description\": \"recipe to cook the cusine\", \"type\": \"string\"}}, \"required\": [\"name\", \"recipe\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{user_input}\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | output_parser\n",
        "\n",
        "chain.invoke({\"user_input\":\"Let me know how to cook Bibimbap\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQwxcR0pzlJH",
        "outputId": "50303f3c-2e57-42b1-9151-169eba88ede8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Bibimbap',\n",
              " 'recipe': 'To cook Bibimbap, start by preparing the ingredients: cook rice, and prepare a variety of vegetables such as spinach, carrots, zucchini, and bean sprouts by sautéing them separately. You can also add mushrooms and other vegetables to your preference. For protein, you can include bulgogi (marinated beef), tofu, or a fried egg on top. Once all the components are ready, assemble the dish by placing the rice in'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 : https://wikidocs.net/book/14473\n"
      ],
      "metadata": {
        "id": "mdxvEBvznvmH"
      }
    }
  ]
}